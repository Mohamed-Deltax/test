{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3566cf9-d3bc-4701-824a-16ff51c1b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "# This script processes images and corresponding YOLO label files to crop and save specific regions (faces of adults and children) into separate directories.\n",
    "# 1. `read_yolo_labels` function reads YOLO label files and extracts bounding boxes and class ids.\n",
    "# 2. `convert_yolo_to_bbox` converts YOLO format (center coordinates and dimensions) to traditional bounding box format (x_min, y_min, x_max, y_max).\n",
    "# 3. `crop_and_save_image` crops the image based on bounding boxes and saves the cropped regions.\n",
    "# 4. `process_images` iterates through images and their labels, identifies if the bounding box corresponds to an adult (class_id 1) or child (class_id 10), crops, and saves them in respective directories.\n",
    "\n",
    "\n",
    "def read_yolo_labels(label_file):\n",
    "    \"\"\"Read YOLO label file and return a list of bounding boxes and class ids.\"\"\"\n",
    "    with open(label_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        bboxes = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            bboxes.append((class_id, x_center, y_center, width, height))\n",
    "    return bboxes\n",
    "\n",
    "def convert_yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
    "    \"\"\"Convert YOLO format to bounding box format.\"\"\"\n",
    "    class_id, x_center, y_center, width, height = yolo_bbox\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "    return class_id, x_min, y_min, x_max, y_max\n",
    "\n",
    "def crop_and_save_image(img, bbox, output_dir, class_name, img_name, crop_id):\n",
    "    \"\"\"Crop the image using the bounding box and save the cropped image.\"\"\"\n",
    "    _, x_min, y_min, x_max, y_max = bbox\n",
    "    crop_img = img[y_min:y_max, x_min:x_max]\n",
    "    output_path = os.path.join(output_dir, f\"{class_name}_{img_name}_{crop_id}.jpg\")\n",
    "    cv2.imwrite(output_path, crop_img)\n",
    "\n",
    "def process_images(image_dir, label_dir, output_dir_adult, output_dir_child):\n",
    "    \"\"\"Process each image and its corresponding label to crop and save faces.\"\"\"\n",
    "    if not os.path.exists(output_dir_adult):\n",
    "        os.makedirs(output_dir_adult)\n",
    "    if not os.path.exists(output_dir_child):\n",
    "        os.makedirs(output_dir_child)\n",
    "\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        bboxes = read_yolo_labels(label_path)\n",
    "\n",
    "        crop_id = 0\n",
    "        for bbox in bboxes:\n",
    "            class_id, x_min, y_min, x_max, y_max = convert_yolo_to_bbox(bbox, img_width, img_height)\n",
    "\n",
    "            if class_id == 1:\n",
    "                crop_and_save_image(img, (class_id, x_min, y_min, x_max, y_max), output_dir_adult, '1', os.path.splitext(img_name)[0], crop_id)\n",
    "            elif class_id == 10:\n",
    "                crop_and_save_image(img, (class_id, x_min, y_min, x_max, y_max), output_dir_child, '0', os.path.splitext(img_name)[0], crop_id)\n",
    "\n",
    "            crop_id += 1\n",
    "\n",
    "# Define your directories\n",
    "image_dir = '/hdd/eldor/age_dataset/CPD_done/train/images'\n",
    "label_dir = '/hdd/eldor/age_dataset/CPD_done/train/labels'\n",
    "output_dir_adult = '/hdd/eldor/age_dataset/CPD_done/train/faces/1'\n",
    "output_dir_child = '/hdd/eldor/age_dataset/CPD_done/train/faces/0'\n",
    "\n",
    "# Run the processing\n",
    "process_images(image_dir, label_dir, output_dir_adult, output_dir_child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1974772-dc9c-47b7-8a4a-d47a0fcf65d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths and parameters\n",
    "data_dir = '/hdd/eldor/age_dataset/CPD_done/train/faces'\n",
    "output_dir = 'resnet_models'  # Directory to save best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5de326e-1fc3-410f-a4c9-12d2958d987e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eldor/miniconda3/envs/age/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "# This script trains multiple ResNet models (ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152) in parallel using multiple GPUs.\n",
    "# \n",
    "# Key Components:\n",
    "# 1. **Data Preparation**: \n",
    "#    - The dataset is loaded from a directory, and the images are preprocessed using transformations (resize, normalize).\n",
    "#    - The dataset is split into training, validation, and test sets, ensuring stratified splits based on class labels.\n",
    "# \n",
    "# 2. **Model Modification**: \n",
    "#    - The fully connected (FC) layers of the pre-trained ResNet models are modified to suit the binary classification task (adult vs. child).\n",
    "# \n",
    "# 3. **Training Process**: \n",
    "#    - Each ResNet model is trained in parallel on different GPUs using multiprocessing, optimizing the network over a defined number of epochs.\n",
    "#    - During training, the model evaluates performance on both training and validation sets, and the best-performing model is saved.\n",
    "# \n",
    "# 4. **Evaluation**: \n",
    "#    - After training, each model is evaluated on the test set, and key metrics like accuracy, precision, recall, F1-score, and confusion matrix are computed and displayed.\n",
    "# \n",
    "# 5. **Parallelism**: \n",
    "#    - The training is distributed across available GPUs using multiprocessing, allowing models to train concurrently and efficiently.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import models\n",
    "import copy\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure 'spawn' is set for multiprocessing (important for Windows and other environments)\n",
    "try:\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Paths and parameters\n",
    "data_dir = '/hdd/eldor/age_dataset/CPD_done/train/faces'\n",
    "output_dir = 'resnet_models'  # Directory to save best models\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2  # Assuming binary classification (e.g., adult vs. child)\n",
    "epochs = 30\n",
    "\n",
    "# Define the ResNet architectures\n",
    "resnet_variants = {\n",
    "    'resnet18': models.resnet18(pretrained=True),\n",
    "    'resnet34': models.resnet34(pretrained=True),\n",
    "    'resnet50': models.resnet50(pretrained=True),\n",
    "    'resnet101': models.resnet101(pretrained=True),\n",
    "    'resnet152': models.resnet152(pretrained=True)\n",
    "}\n",
    "\n",
    "# GPUs (You only have GPU 0 and GPU 1)\n",
    "gpu_ids = [0, 1]\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train, validation, and test sets with stratification\n",
    "train_indices, val_test_indices = train_test_split(\n",
    "    np.arange(len(dataset.targets)),\n",
    "    test_size=0.3,\n",
    "    stratify=dataset.targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    val_test_indices,\n",
    "    test_size=0.5,\n",
    "    stratify=[dataset.targets[i] for i in val_test_indices],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "test_set = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def modify_resnet_fc(resnet_model, num_classes):\n",
    "    in_features = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    return resnet_model\n",
    "\n",
    "def train_model(resnet_name, resnet_model, gpu_id, output_dir, results):\n",
    "    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training {resnet_name} on {device}...\")\n",
    "\n",
    "    resnet_model = modify_resnet_fc(resnet_model, num_classes)\n",
    "    resnet_model = resnet_model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "\n",
    "    best_model_wts = copy.deepcopy(resnet_model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                resnet_model.train()\n",
    "            else:\n",
    "                resnet_model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            with tqdm(total=len(dataloaders[phase]), desc=f'{phase.capitalize()} {resnet_name}', ncols=100) as pbar:\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = resnet_model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(resnet_model.state_dict())\n",
    "    \n",
    "    resnet_model.load_state_dict(best_model_wts)\n",
    "\n",
    "    model_save_path = os.path.join(output_dir, f\"{resnet_name}_best_model.pth\")\n",
    "    torch.save(resnet_model.state_dict(), model_save_path)\n",
    "    print(f\"Best model saved for {resnet_name} at {model_save_path}\")\n",
    "    \n",
    "    accuracy, precision, recall, f1, conf_matrix = evaluate_model(resnet_model, test_loader, device)\n",
    "    \n",
    "    result = {\n",
    "        'Model': resnet_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "def evaluate_model(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "def run_parallel_training():\n",
    "    processes = []\n",
    "    results = multiprocessing.Manager().list()\n",
    "\n",
    "    model_pairs = [\n",
    "        ('resnet18', 'resnet34'),\n",
    "        ('resnet50', 'resnet101'),\n",
    "        ('resnet152', None)\n",
    "    ]\n",
    "\n",
    "    for model1, model2 in model_pairs:\n",
    "        if model1:\n",
    "            p1 = multiprocessing.Process(target=train_model, args=(model1, resnet_variants[model1], gpu_ids[0], output_dir, results))\n",
    "            p1.start()\n",
    "            processes.append(p1)\n",
    "        \n",
    "        if model2:\n",
    "            p2 = multiprocessing.Process(target=train_model, args=(model2, resnet_variants[model2], gpu_ids[1], output_dir, results))\n",
    "            p2.start()\n",
    "            processes.append(p2)\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"Metrics for {result['Model']}:\")\n",
    "        for metric_name, metric_value in result.items():\n",
    "            if metric_name != 'Model':\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_parallel_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e44bb-8b24-4c44-b33a-0003d392c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy:\n",
    "#     resnet18: 0.9954\n",
    "#     resnet34: 0.9947\n",
    "#     resnet50: 0.9894\n",
    "#     resnet101:\n",
    "#     resnet152:0.9930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da42a556-4ba5-46ac-8af3-36ebbf4dec3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating resnet18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for resnet18:\n",
      "Accuracy: 0.9970\n",
      "Precision: 0.9970\n",
      "Recall: 0.9970\n",
      "F1-Score: 0.9970\n",
      "Confusion Matrix:\n",
      "[[1164    0]\n",
      " [   9 1845]]\n",
      "\n",
      "\n",
      "Evaluating resnet34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for resnet34:\n",
      "Accuracy: 0.9964\n",
      "Precision: 0.9964\n",
      "Recall: 0.9964\n",
      "F1-Score: 0.9964\n",
      "Confusion Matrix:\n",
      "[[1161    3]\n",
      " [   8 1846]]\n",
      "\n",
      "\n",
      "Evaluating resnet50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for resnet50:\n",
      "Accuracy: 0.9960\n",
      "Precision: 0.9960\n",
      "Recall: 0.9960\n",
      "F1-Score: 0.9960\n",
      "Confusion Matrix:\n",
      "[[1161    3]\n",
      " [   9 1845]]\n",
      "\n",
      "\n",
      "Evaluating resnet101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for resnet101:\n",
      "Accuracy: 0.9973\n",
      "Precision: 0.9974\n",
      "Recall: 0.9973\n",
      "F1-Score: 0.9974\n",
      "Confusion Matrix:\n",
      "[[1163    1]\n",
      " [   7 1847]]\n",
      "\n",
      "\n",
      "Evaluating resnet152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eldor/miniconda3/envs/age/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for resnet152:\n",
      "Accuracy: 0.9954\n",
      "Precision: 0.9954\n",
      "Recall: 0.9954\n",
      "F1-Score: 0.9954\n",
      "Confusion Matrix:\n",
      "[[1157    7]\n",
      " [   7 1847]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This script evaluates several pre-trained ResNet models (ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152) on a binary classification task (e.g., adult vs. child).\n",
    "# \n",
    "# Key Components:\n",
    "# \n",
    "# 1. **Data Loading and Preprocessing**:\n",
    "#    - The dataset is loaded from a specified directory using `ImageFolder`, and images are resized, normalized, and converted to tensors.\n",
    "#    - A test subset is created using a predefined train/test split to ensure consistency with the training process.\n",
    "# \n",
    "# 2. **Model Modification**:\n",
    "#    - The fully connected (FC) layer of each pre-trained ResNet model is replaced with a new architecture tailored to binary classification. The FC layers include intermediate layers with ReLU activations for feature learning.\n",
    "# \n",
    "# 3. **Model Evaluation**:\n",
    "#    - The script loads each ResNet model from saved checkpoints (if available) and evaluates it on the test set.\n",
    "#    - Metrics such as accuracy, precision, recall, F1-score, and the confusion matrix are calculated for each model to provide insights into the model's performance.\n",
    "# \n",
    "# 4. **Evaluation Loop**:\n",
    "#    - The script iterates over a set of ResNet models, loading each model's state from disk, modifying the architecture for the classification task, and computing evaluation metrics.\n",
    "#    - If a model file is not found, the script reports it without halting execution.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Paths\n",
    "output_dir = 'resnet_models'\n",
    "data_dir = '/hdd/eldor/age_dataset/CPD_done/train/faces'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "num_classes = 2  # Assuming binary classification (e.g., adult vs. child)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Assuming the same train/test split as in training\n",
    "_, val_test_indices = train_test_split(\n",
    "    np.arange(len(dataset.targets)),\n",
    "    test_size=0.3,\n",
    "    stratify=dataset.targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "_, test_indices = train_test_split(\n",
    "    val_test_indices,\n",
    "    test_size=0.5,\n",
    "    stratify=[dataset.targets[i] for i in val_test_indices],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_set = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Function to modify the model's fully connected layer\n",
    "def modify_resnet_fc(resnet_model, num_classes):\n",
    "    in_features = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    return resnet_model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Evaluate all saved models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_names = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = os.path.join(output_dir, f\"{model_name}_best_model.pth\")\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "        # Load the model\n",
    "        resnet_model = getattr(models, model_name)(pretrained=False)\n",
    "        resnet_model = modify_resnet_fc(resnet_model, num_classes)\n",
    "        resnet_model.load_state_dict(torch.load(model_path))\n",
    "        resnet_model = resnet_model.to(device)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy, precision, recall, f1, conf_matrix = evaluate_model(resnet_model, test_loader, device)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Metrics for {model_name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Model {model_name} not found at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558c3bad-b687-40bf-8558-a0fac7d3a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metrics for resnet18:\n",
    "# Accuracy: 0.9970\n",
    "# Precision: 0.9970\n",
    "# Recall: 0.9970\n",
    "# F1-Score: 0.9970\n",
    "\n",
    "# Metrics for resnet34:\n",
    "# Accuracy: 0.9964\n",
    "# Precision: 0.9964\n",
    "# Recall: 0.9964\n",
    "# F1-Score: 0.9964\n",
    "\n",
    "# Metrics for resnet50:\n",
    "# Accuracy: 0.9960\n",
    "# Precision: 0.9960\n",
    "# Recall: 0.9960\n",
    "# F1-Score: 0.9960\n",
    "\n",
    "# Metrics for resnet101:\n",
    "# Accuracy: 0.9973\n",
    "# Precision: 0.9974\n",
    "# Recall: 0.9973\n",
    "# F1-Score: 0.9974\n",
    "\n",
    "\n",
    "# Metrics for resnet152:\n",
    "# Accuracy: 0.9954\n",
    "# Precision: 0.9954\n",
    "# Recall: 0.9954\n",
    "# F1-Score: 0.9954\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1984f7-2eae-49a7-950b-b59dab2b7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the models on a video examples\n",
    "\n",
    "# import cv2\n",
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision import models\n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "# from facenet_pytorch import MTCNN\n",
    "# from PIL import Image\n",
    "\n",
    "# # Initialize MTCNN for face detection\n",
    "# mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Parameters\n",
    "# input_video_path = 'test_3.mp4'\n",
    "# output_dir = 'resnet_output_video'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# model_dir = 'resnet_models'\n",
    "# batch_size = 32\n",
    "# num_classes = 2  # Assuming binary classification (e.g., adult vs. child)\n",
    "\n",
    "# # Define transforms for the face images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Function to modify the model's fully connected layer\n",
    "# def modify_resnet_fc(resnet_model, num_classes):\n",
    "#     in_features = resnet_model.fc.in_features\n",
    "#     resnet_model.fc = nn.Sequential(\n",
    "#         nn.Linear(in_features, 512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(512, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     return resnet_model\n",
    "\n",
    "# # Load models\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model_names = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     model_path = os.path.join(model_dir, f\"{model_name}_best_model.pth\")\n",
    "    \n",
    "#     if os.path.exists(model_path):\n",
    "#         print(f\"Processing video for {model_name}...\")\n",
    "\n",
    "#         # Load the model\n",
    "#         resnet_model = getattr(models, model_name)(pretrained=False)\n",
    "#         resnet_model = modify_resnet_fc(resnet_model, num_classes)\n",
    "#         resnet_model.load_state_dict(torch.load(model_path))\n",
    "#         resnet_model = resnet_model.to(device)\n",
    "#         resnet_model.eval()\n",
    "\n",
    "#         # Process the video\n",
    "#         cap = cv2.VideoCapture(input_video_path)\n",
    "#         fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#         output_video_path = os.path.join(output_dir, f\"{model_name}_output_video.avi\")\n",
    "#         out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "#         while cap.isOpened():\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#             # Use MTCNN to detect faces\n",
    "#             boxes, _ = mtcnn.detect(frame)\n",
    "\n",
    "#             if boxes is not None:\n",
    "#                 for box in boxes:\n",
    "#                     x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "#                     # Ensure the bounding box is within the frame boundaries\n",
    "#                     x1 = max(0, x1)\n",
    "#                     y1 = max(0, y1)\n",
    "#                     x2 = min(frame.shape[1], x2)\n",
    "#                     y2 = min(frame.shape[0], y2)\n",
    "\n",
    "#                     # Check if the bounding box is valid\n",
    "#                     if x2 > x1 and y2 > y1:\n",
    "#                         face_img = frame[y1:y2, x1:x2]\n",
    "                        \n",
    "#                         # Convert the NumPy array (OpenCV image) to a PIL image\n",
    "#                         face_img_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "                        \n",
    "#                         # Apply the transformations\n",
    "#                         face_tensor = transform(face_img_pil)\n",
    "#                         face_tensor = face_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "#                         # Get prediction from the model\n",
    "#                         with torch.no_grad():\n",
    "#                             output = resnet_model(face_tensor)\n",
    "#                             _, predicted_class = torch.max(output, 1)\n",
    "#                             label = 'Child' if predicted_class.item() == 0 else 'Adult'\n",
    "\n",
    "#                         # Draw bounding box and label\n",
    "#                         color = (0, 255, 0) if label == 'Adult' else (255, 0, 0)\n",
    "#                         cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "#                         cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "#             # Write the frame to the output video\n",
    "#             out.write(frame)\n",
    "\n",
    "#         cap.release()\n",
    "#         out.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         print(f\"Processed video saved to {output_video_path}\")\n",
    "#     else:\n",
    "#         print(f\"Model {model_name} not found at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2dd81-f807-4887-a3b6-0827b7723724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6a606-361c-408e-b40d-a1dd8b4221ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f35b6e-b173-42bc-b8a5-59b103639b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pycocotools.coco import COCO\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# # COCO body keypoint connections (skeleton) in (start, end) format\n",
    "# COCO_SKELETON = [\n",
    "#     (5, 6), (5, 7), (6, 8), (7, 9), (8, 10), \n",
    "#     (5, 11), (6, 12), (11, 12), (11, 13), \n",
    "#     (12, 14), (13, 15), (14, 16)\n",
    "# ]\n",
    "\n",
    "# # A set of distinct colors for classes\n",
    "# COLORS = [\n",
    "#     (255, 0, 0),  # Red\n",
    "#     (0, 255, 0),  # Green\n",
    "#     (0, 0, 255),  # Blue\n",
    "#     (255, 255, 0),  # Cyan\n",
    "#     (255, 0, 255),  # Magenta\n",
    "#     (0, 255, 255),  # Yellow\n",
    "#     (128, 0, 128),  # Purple\n",
    "#     (128, 128, 0),  # Olive\n",
    "#     (0, 128, 128),  # Teal\n",
    "# ]\n",
    "\n",
    "# # Color for keypoints (distinct from class colors)\n",
    "# KEYPOINT_COLOR = (255, 165, 0)  # Orange\n",
    "\n",
    "# def get_class_color(class_id, num_classes):\n",
    "#     \"\"\"Assign a color to a class based on the class ID.\"\"\"\n",
    "#     return COLORS[class_id % len(COLORS)]  # Cycle through available colors\n",
    "\n",
    "# def draw_keypoints(image, keypoints, skeleton=COCO_SKELETON, color=KEYPOINT_COLOR, line_color=KEYPOINT_COLOR):\n",
    "#     \"\"\"Draw keypoints and connect them with lines.\"\"\"\n",
    "#     # Draw connections (skeleton lines)\n",
    "#     for connection in skeleton:\n",
    "#         start_idx, end_idx = connection\n",
    "#         x1, y1, v1 = keypoints[start_idx * 3:(start_idx + 1) * 3]\n",
    "#         x2, y2, v2 = keypoints[end_idx * 3:(end_idx + 1) * 3]\n",
    "        \n",
    "#         # Only draw the connection if both keypoints are visible (v > 0)\n",
    "#         if v1 > 0 and v2 > 0:\n",
    "#             cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), line_color, 2)\n",
    "    \n",
    "#     # Draw keypoints themselves\n",
    "#     for i in range(0, len(keypoints), 3):\n",
    "#         x, y, v = keypoints[i:i+3]\n",
    "#         if v > 0:  # Only draw visible keypoints\n",
    "#             cv2.circle(image, (int(x), int(y)), 3, color, -1)\n",
    "    \n",
    "#     return image\n",
    "\n",
    "# def draw_bbox(image, bbox, color):\n",
    "#     \"\"\"Draw bounding box on the image.\"\"\"\n",
    "#     x, y, w, h = bbox\n",
    "#     cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
    "#     return image\n",
    "\n",
    "# def visualize_annotations(image_path, keypoints_json, bbox_json, output_path):\n",
    "#     # Initialize COCO objects for keypoints and bbox\n",
    "#     coco_keypoints = COCO(keypoints_json)\n",
    "#     coco_bbox = COCO(bbox_json)\n",
    "\n",
    "#     # Get category IDs and assign unique colors to each class\n",
    "#     categories = coco_bbox.loadCats(coco_bbox.getCatIds())\n",
    "#     num_classes = len(categories)\n",
    "#     class_colors = {cat['id']: get_class_color(i, num_classes) for i, cat in enumerate(categories)}\n",
    "\n",
    "#     # Get image IDs from both files (assuming they contain the same image IDs)\n",
    "#     img_ids = coco_keypoints.getImgIds()\n",
    "\n",
    "#     for img_id in img_ids:\n",
    "#         img_info = coco_keypoints.loadImgs(img_id)[0]\n",
    "#         image = cv2.imread(image_path + img_info['file_name'])\n",
    "#         if image is None:\n",
    "#             print(f\"Image {img_info['file_name']} not found!\")\n",
    "#             continue\n",
    "\n",
    "#         # Create copies of the image for separate visualizations\n",
    "#         image_bbox = image.copy()\n",
    "#         image_keypoints = image.copy()\n",
    "\n",
    "#         # Get annotations for keypoints and bounding boxes for the same image\n",
    "#         keypoint_ann_ids = coco_keypoints.getAnnIds(imgIds=img_info['id'])\n",
    "#         bbox_ann_ids = coco_bbox.getAnnIds(imgIds=img_info['id'])\n",
    "\n",
    "#         keypoint_anns = coco_keypoints.loadAnns(keypoint_ann_ids)\n",
    "#         bbox_anns = coco_bbox.loadAnns(bbox_ann_ids)\n",
    "\n",
    "#         # Draw keypoints with connections on keypoint image\n",
    "#         for ann in keypoint_anns:\n",
    "#             if 'keypoints' in ann:\n",
    "#                 image_keypoints = draw_keypoints(image_keypoints, ann['keypoints'])\n",
    "\n",
    "#         # Draw bounding boxes with class-specific colors on bbox image\n",
    "#         for ann in bbox_anns:\n",
    "#             if 'bbox' in ann:\n",
    "#                 class_id = ann['category_id']\n",
    "#                 class_color = class_colors.get(class_id, (255, 255, 255))  # Default to white if class color not found\n",
    "#                 image_bbox = draw_bbox(image_bbox, ann['bbox'], class_color)\n",
    "\n",
    "#         # Save the image with bounding boxes\n",
    "#         bbox_output_file = output_path + \"bbox_\" + img_info['file_name']\n",
    "#         cv2.imwrite(bbox_output_file, image_bbox)\n",
    "#         print(f\"Saved bounding box image: {bbox_output_file}\")\n",
    "\n",
    "#         # Save the image with keypoints\n",
    "#         keypoints_output_file = output_path + \"keypoints_\" + img_info['file_name']\n",
    "#         cv2.imwrite(keypoints_output_file, image_keypoints)\n",
    "#         print(f\"Saved keypoints image: {keypoints_output_file}\")\n",
    "\n",
    "# # Example usage\n",
    "# image_path = \"sample_data/\"\n",
    "# keypoints_json = \"body_keypoints_coco.json\"\n",
    "# bbox_json = \"bbox_coco.json\"\n",
    "# output_path = \"sample_data_output/\"\n",
    "# visualize_annotations(image_path, keypoints_json, bbox_json, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988aa63b-06a7-4e52-be62-391046a20eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/home/eldor/miniconda3/envs/age/bin/python -m pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cedbd8-e312-4a85-89b7-bd77ee1397a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c98509-cd30-40be-af76-208fd831eea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# def calculate_trim_duration(input_file, target_size_mb=100):\n",
    "#     # Get the file size in MB\n",
    "#     current_size_mb = os.path.getsize(input_file) / (1024 * 1024)\n",
    "\n",
    "#     # Get the duration of the video using ffprobe\n",
    "#     result = subprocess.run(\n",
    "#         ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', input_file],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.STDOUT\n",
    "#     )\n",
    "#     duration = float(result.stdout)\n",
    "    \n",
    "#     print(f\"Current video size: {current_size_mb:.2f} MB\")\n",
    "#     print(f\"Video duration: {duration:.2f} seconds\")\n",
    "\n",
    "#     # Calculate the average bitrate (size per second)\n",
    "#     average_bitrate = current_size_mb / duration\n",
    "#     print(f\"Average bitrate: {average_bitrate:.6f} MB/second\")\n",
    "\n",
    "#     # Calculate the duration needed to make the video less than the target size\n",
    "#     target_duration = target_size_mb / average_bitrate\n",
    "#     trim_duration = duration - target_duration\n",
    "\n",
    "#     if trim_duration > 0:\n",
    "#         print(f\"You need to trim the video by {trim_duration:.2f} seconds to make it less than {target_size_mb} MB.\")\n",
    "#     else:\n",
    "#         print(f\"The video is already smaller than {target_size_mb} MB, no need to trim.\")\n",
    "    \n",
    "#     return duration, trim_duration if trim_duration > 0 else 0\n",
    "\n",
    "# def trim_video_ffmpeg(input_file, output_file, start_time, duration):\n",
    "#     # Construct the ffmpeg command to trim the video\n",
    "#     ffmpeg_command = [\n",
    "#         'ffmpeg', '-i', input_file, '-ss', str(start_time), '-t', str(duration),\n",
    "#         '-c', 'copy', output_file\n",
    "#     ]\n",
    "\n",
    "#     # Run the ffmpeg command\n",
    "#     subprocess.run(ffmpeg_command)\n",
    "\n",
    "#     # Check the final size\n",
    "#     if os.path.exists(output_file):\n",
    "#         final_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "#         print(f\"Trimmed video size: {final_size_mb:.2f} MB\")\n",
    "#     else:\n",
    "#         print(f\"Trimming failed. The output file was not created.\")\n",
    "\n",
    "# # Example usage\n",
    "# input_video_path = 'teleian_sportage_daytime_2024_08_26.mp4'\n",
    "# output_video_path = 'teleian_sportage_daytime_2024_08_26_compressed.mp4'\n",
    "\n",
    "# # Calculate how many seconds to trim\n",
    "# duration, trim_duration = calculate_trim_duration(input_video_path, target_size_mb=90)\n",
    "\n",
    "# # If trimming is needed, cut the video\n",
    "# if trim_duration > 0:\n",
    "#     trim_video_ffmpeg(input_video_path, output_video_path, start_time=0, duration=(duration - trim_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5c66b-7e34-4d44-a742-e9d3ce8e0edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ebdbf-2ec1-497f-999d-ce4c6b66317d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b52e95-2142-4660-b542-2bfafd13d492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484196b-385f-4c25-87c6-c1606054b4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
